---
title: Evaluate LLMs
description: sample page
custom_edit_url: null
---

# Evaluate Watsonx.ai models
## Prerequisites
- Create a watsonx.ai instance

## Instructions
- Write a prompt in prompt lab.
![](./Instruction-Screenshots/EmailGenerationPrompt.png)
- Save as a prompt template.
![](./Instruction-Screenshots/PromptTemplate.png)
- Promote prompt to space
![](./Instruction-Screenshots/EmailGenerationPrompteToSpace.png)
- Deploy 
![](./Instruction-Screenshots/PromptDeployment.png)
- View in AI Factsheet and Track in AI Use Case
![](./Instruction-Screenshots/AIFactsheet.png)
- Evaluate using the payload and/or feedback datasets
- Example dataset:
    - [Resume Extraction example](../../assets/datasets/Resume Extraction example.csv)
    - [Resume Summarization feedback data example](../../assets/datasets/Resume Summarization feedback data example.csv)
    - [Resume Summarization payload data example](../../assets/datasets/Resume Summarization payload data example.csv)

## Metrics Interpretation
- Extraction
    - ROUGE scores
![](./Instruction-Screenshots/ResumeExtractionMetrics.png)
- Generation
    - Readability score
    - HAP score
![](./Instruction-Screenshots/EmailGenerationMetrics.png)

- Summarization
    - ROUGE scores
![](./Instruction-Screenshots/ResumeSummarizationMetrics.png)


## Reference: 
- [Configuring generative AI quality evaluations](https://dataplatform.cloud.ibm.com/docs/content/wsj/model/wos-monitor-gen-quality.html?context=wx&audience=wdp#supported-generative-ai-quality-metrics)
